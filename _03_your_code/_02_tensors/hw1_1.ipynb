{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 2019136011 권성민 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afd5667754111afe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## a_tensor_initialization.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cf92569d9fe1576"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T08:53:00.959200100Z",
     "start_time": "2023-09-17T08:52:59.758897200Z"
    }
   },
   "id": "d9ee17edb9f466f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch package를 현재 prompt에 include한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d441fce542b0066b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T08:53:40.649044800Z",
     "start_time": "2023-09-17T08:53:40.633046100Z"
    }
   },
   "id": "4b2359aa09c7698"
  },
  {
   "cell_type": "markdown",
   "source": [
    "python의 [1,2,3] 리스트를 torch의 Tensor 객체로 CPU에서 사용한다.\n",
    "torch의 Tensor 객체는 기본 값 데이터 자료형으로 torch.float32을 사용한다.\n",
    "device는 cpu 혹은 gpu에서 작동을 하는데 cpu에서 작동한다는 것을 나타낸다.\n",
    "requires_grad는 pyTorch의 자동 미분 엔진을 사용하는지 여부를 나타낸다. 현재는 기본 값인 False로 사용하지 않는다.\n",
    "size()로는 [3]으로 생성자의 인자가 [1,2,3]이므로 1차원 tensor에 요소로 3개가 있다는 뜻이다.\n",
    "shape는 size()와 동일한 값으로, 멤버 변수로 불러올 수도 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1cea85c5e56430a"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#if you have gpu device\n",
    "t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "t1_cuda = t1.cuda()\n",
    "t1_cpu = t1.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:20:11.779496600Z",
     "start_time": "2023-09-17T10:20:11.761698500Z"
    }
   },
   "id": "c504ade49f1736e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "t1.to(torch.device('cuda')) 와 t1.cuda()의 방법으로 RAM 메모리에 있는 값을 GPU의 메모리로 옮긴다.\n",
    "반대로 t1.cpu()와 같이 멤버 함수로 GPU에서 RAM으로 옮길 수도 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57d09cbf6382a10e"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:20:12.925418Z",
     "start_time": "2023-09-17T10:20:12.899988Z"
    }
   },
   "id": "faf34b9e08a76c3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "a1과 a2를 비교해보면 a1은 값 자체를 넘겨주고, a2는 리스트 형태로 넘겨준다.\n",
    "따라서 a1의 차원은 0차원으로 점의 형태이고, a2의 경우 1차원 tensor이며 element는 1개가 있다.. \n",
    "a3의 경우 1차원 tensor로 elements가 5개이다.\n",
    "a4의 경우 2차원 tensor로, 1차원 tensor의 집합을 tensor로 갖으므로 2차원 tensor이다.\n",
    "a5의 겨웅 마찬가지로 2차원 tensor로 elements가 2개인 1차원 tensor 3개를 array로 갖는 2차원 tensor이다.\n",
    "a6의 경우 3차원 tensor이다. 1개의 element를 요소로 갖는 1차원 tensors가 있고 이러한 1차원 tensors를 2개씩 element로 갖는 2차원 tensors와, 2차원 tensors를 3개를 element로 갖는 3차원 tensor가 바로 a6이다.\n",
    "a7의 경우 4차원 tensor로 a6와 유사한 방식으로 볼 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a65a3cb4010d7f13"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 32\u001B[0m\n\u001B[0;32m     24\u001B[0m a11 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([                 \u001B[38;5;66;03m# shape: torch.Size([4, 1, 5]), ndims(=rank): 3\u001B[39;00m\n\u001B[0;32m     25\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     26\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     27\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     28\u001B[0m     [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m]],\n\u001B[0;32m     29\u001B[0m ])\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(a11\u001B[38;5;241m.\u001B[39mshape, a11\u001B[38;5;241m.\u001B[39mndim)\n\u001B[1;32m---> 32\u001B[0m a12 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m                 \u001B[49m\u001B[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001B[39;49;00m\n\u001B[0;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     35\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a11.shape, a11.ndim)\n",
    "\n",
    "a12 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:20:14.133993800Z",
     "start_time": "2023-09-17T10:20:14.096994400Z"
    }
   },
   "id": "96e552b0328ab6bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "a8은 4차원 tensor로 위 주석에서 설명한 것과 유사하게 확인할 수 있다. 이러한 복잡한 tensor의 경우 ']'가 나오기 이전까지의 '['의 갯수를 세어서 차원을 추측할 수 있다. \n",
    "a9도 위에서 설명한 방법을 기반으로 ']'가 나오기 이전에 '['가 [[[[[으로 5개이므로 5차원 tensor이다.\n",
    "a10은 2차원 tensor로 5개의 elements를 갖는 1차원 tensor가 4개로, [4,5] shape의 2차원 tensor이다.\n",
    "a11은 3차원 tensor로 [의 수를 통해 확인할 수 있다. 5개의 elements를 갖느 1차원 tensor를 element로 1개씩 있는 2차원 tensor와, 이러한 tensor를 4개로 갖는 3차원 tensor가 a11이다. 따라서 shape는 [4,1,5]이다.\n",
    "a12는 오류가 발생하는 경우로 왜냐하면 tensor는 array이기 때문이다. array라 함은 각 차원에서 요소가 동일해야 한다. 따라서 [1,2,3]을 [1,2]로 바꾸던가 [4,5]를 [4,5,6]으로 변경해야 한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "140f3a764a308090"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## b_tensor_initialization.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3fd28a9b84ae711"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "print(\"#\" * 100)\n",
    "\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:20:15.554729300Z",
     "start_time": "2023-09-17T10:20:15.532730100Z"
    }
   },
   "id": "3f377b021621f8fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "해당 prompt에서는 torch와 numpy를 사용하며 numpy와 python list를 tensor로 변경하는 예제이다.\n",
    "l1, l2, l3은 python의 list를 tensor로 변경하는 방법을 예제로 보여준다.\n",
    "결과를 보면 torch.Tensor로 생성하는 경우 생성자를 통해 생성하므로 Tensor의 기본 자료형인 torch.FloatTensor로 생성되는 것을 볼 수 있다.\n",
    "이에 반해서 tensor와 as_tensor 멤버 함수의 경우 입력된 값의 데이터 자료형을 받아서 생성하므로 torch.IntTensor로 생성되는 것을 볼 수 있다.\n",
    "또한 원래 자료형의 값을 변경했음에도 tensor의 값은 변경되지 않는다. 이는 모두 deep copy를 사용한다는 것을 볼 수 있다.\n",
    "\n",
    "l4,l5,l6은 numpy의 array를 tensor로 만드는 과정이다.\n",
    "마찬가지로 torch.Tensor는 생성자이므로, 데이터 자료형을 Float로 만드는 반면에 tensor와 a_tensor는 Int 자료형으로 만들어지는 것을 볼 수 있다.\n",
    "이때 주의 해야 할 점으로 numpy array를 as_tensor로 만드는 경우 shallow copy가 발생한다는 점을 볼 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1301ad2cbae11e4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## c_tensor_initialization_constant_values.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c80429ea29870fb"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.4013e-45, 0.0000e+00, 4.0627e-12, 8.1836e-43])\n",
      "tensor([1.4013e-45, 0.0000e+00, 4.0613e-12, 8.1836e-43])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:20:16.863991500Z",
     "start_time": "2023-09-17T10:20:16.843998Z"
    }
   },
   "id": "8009e678a9c0338f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch.ones는 주어진 size에 맞춰서 array를 만든다. 이때 1로 초기화하는 것을 의미한다. 따라서 [5,]의 size로 들어왔기 때문에 [1,1,1,1,1]로 1차원 tensor를 만든다. ones_like는 입력된 tensor와 동일한 size의 tensor를 1로 초기화하여 반환한다.\n",
    "\n",
    "zeros와 zeros_like는 ones와 다르게 0으로 초기화하여 반환한다.\n",
    "\n",
    "empty 또한 ones와 동일하지만, 쓰레기 값이 들어간다.\n",
    "\n",
    "eye의 경우 n x n 크기의 Identity Matrix를 만들어 반환한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3ca7475684a9b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## d_tensor_initialization_random_values.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "662d169f8862e4e2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16, 17]])\n",
      "tensor([[0.8083, 0.4360, 0.1718]])\n",
      "tensor([[-0.1120,  0.1259,  0.2672]])\n",
      "tensor([[11.1580, 10.5840],\n",
      "        [10.0479,  8.5969],\n",
      "        [ 9.3034, 10.5792]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T09:35:39.464825800Z",
     "start_time": "2023-09-17T09:35:39.416763100Z"
    }
   },
   "id": "95a81ecbf44684d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch.randint는 [10,20)의 랜덤 값을 갖으며, tensor의 shape는 size로 들어온 [1,2]의 2차원 tensor를 만든다.\n",
    "이에 반해 torch.rand는 [0,1] 사이의 실수를 size 인자로 들어온 [1,3]의 2차원 tensor를 만든다.\n",
    "torch.randn은 표준 정규 분포에서 sampling한 값을 꺼내며, 평균은 0, 분산은 1인 [0,1] 범위에서 들어온 size 인자에 맞춰서 tensor를 반환한다.\n",
    "torch.normal은 정규 분포에서 sampling한 값을 꺼내며 평균과 분산은 인자로 들어온 값에 따라 달라진다.\n",
    "torch.linspace는 1차원 tensor를 반환한다. 이때 인자로 범위가 주어지며, 인자인 steps에 따라서 해당 범위를 steps만큼의 n등분하여 각 값을 반환한다.\n",
    "torch.arrange는 1차원 tensor를 반환하며, 인자로 들어온 값으로 [0, argument)까지 반환한다.\n",
    "\n",
    "torch.manual_seed는 random을 위한 seed로 랜덤은 seed에서 복잡한 연산을 통해 중복되지 않는 값을 random으로 뽑는게 원리이므로, seed를 고정하여 같은 순서의 랜덤 값으로 나오게 한다.\n",
    "torch.rand를 통해 seed를 동일하게 설정 시 같은 값이 나오는 것을 볼 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdbb57525ff7c264"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## e_tensor_type_conversion.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1704ee786a55e8be"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)\n",
    "\n",
    "d = b.to(torch.int32)\n",
    "print(d)\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T09:49:06.045515200Z",
     "start_time": "2023-09-17T09:49:06.035520500Z"
    }
   },
   "id": "a1ce807a51825272"
  },
  {
   "cell_type": "markdown",
   "source": [
    "dtype 인자를 사용하여서 원하는 a, b 변수를 통해 볼 수 있다. Tensor가 기본적으로 torch.float32이지만, int16으로 dtype 인자를 주어 2바이트 int형으로 만든다.\n",
    "c는 rand로 float64 데이터 자료형으로 [0,1] 범위의 반환을 한다. 이때 20을 곱하는데 broadcast로 [0,20]의 범위의 rand으로 반환한다.\n",
    "d는 2바이트 int형을 4바이트 int형으로 변경하여 반환하는 것을 볼 수 있다.\n",
    "\n",
    "print(short_e.dtype) 까지의 코드는 인자로 주는 방법 외에도 멤버 함수를 통해서 다른 타입으로 변경하는 것을 보여준다. ( .데이터자료형 , .to(데이터자료형) )\n",
    "그 다음 예제는 다른 데이터 자료형끼리의 연산으로 일반적으로 다른 언어에서와 마찬가지로 더 큰 자료형의 데이터 타입으로 변경해서 연산하는 것을 볼 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2db51ce58890ffe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## f_tensor_operations.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a073eac8dd85b7b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T09:58:41.938406600Z",
     "start_time": "2023-09-17T09:58:41.923413500Z"
    }
   },
   "id": "d565874fcf06de5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "첫 번째 예제는 명시적으로 torch의 멤버 함수를 통해 더하는 방법과, operation override된 operator를 사용하여 연산하는 것을 보여준다. 각 값은 같은 것을 볼 수 있다.\n",
    "\n",
    "2번째 예제는 add가 아닌 substract에 대한 예제로 add와 동일한 설명이다.\n",
    "3번째 예제는 multiply로 곱 연산에 대한 예제로 add와 동일한 설명이다.\n",
    "4번째 예제는 division에 대한 예제로 add와 동일한 설명이다.\n",
    "\n",
    "여기서의 연산은 같은 위치의 elements 사이의 연산을 의미한다.\n",
    "또한 broadcasting 연산이 없다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb2427a5b0f74bbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## g_tensor_operations.mm.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19008db2fe0ca69"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:02:02.442203400Z",
     "start_time": "2023-09-17T10:02:02.431206100Z"
    }
   },
   "id": "8b598bed5a75fe11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "각 예제는 선형대수에 대한 예제이다.\n",
    "첫 번째 dot product 연산은 보면 shape가 다른 tensor끼리의 dot product를 보여준다. 이때 작은 차원의 tensor가 더 큰 차원의 tensor의 차원(+ element count)에 맞춰서 더 작은 차원의 값을 복사해서 같은 shape로 맞춘 후 dot product 연산을 진행한다. dot product의 연산 결과는 Scalar 값이므로 0차원 tensor가 나온다.\n",
    "\n",
    "2번째 예제는 matrix multiply의 연산으로 행렬연산을 진행한다. 2 x 3 행렬과 3 x 2 행렬의 연산으로 2 x 2 행렬 연산 결과가 나오는 것을 볼 수 있다.\n",
    "여기서 mm은 broadcasting 연산이 없다.\n",
    "\n",
    "3번쨰 예제는 batch matrix multiply의 연산으로 3x4 Matrix와 4 x 5 Matrix를 batch 단위로(10) 연산하여 [10,3,5]의 shape tensor가 나오는 것을 볼 수 있다.\n",
    "여기서 bmm은 broadcastring 연산이 없다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c5c785cc2c534d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## h_tensor_operations_matmul.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5f3830ebdb69ed8"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:06:43.271068600Z",
     "start_time": "2023-09-17T10:06:43.235068700Z"
    }
   },
   "id": "e33724c63efb40e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "첫 번째 예제는 0차원 tensor에 대해서 multiple matrix multiply에 대해 한다. 따라서 0차원 tensor가 결과로 나온다. (dot product)\n",
    "mm과의 차이점으로 braodcasting 연산이 지원된다.\n",
    "\n",
    "두 번째 예제는 3개를 batch로 삼는 t3 tensor(4 elements vector)와 4 elements vector간의 matrix multiply(dot product)를 진행한다.\n",
    "shape가 맞지 않기 때문에 broadcasting이 되어서 t4가 3개의 batch로 broadcasting이 된다. 이때 dot product의 연산으로 Scalar 값이 나오므로 1차원 tensor가 반환된다.\n",
    "\n",
    "세 번째 예제 또한 broadcasting에 대한 결과로 t6이 [10,3,4]로 broadcasting이 되어 [10,3]의 batch를 갖고 dot product 연산이 지원된다.\n",
    "\n",
    "네 번째 예제는 10을 batch로 갖고 3x4 matrix와 4x5 matrix 사이의 연산이 되어 [10,3,5]의 결과가 나온다.\n",
    "\n",
    "다섯 번째 예제는 t10이 broadcasting이 되어 10을 batch로 갖고, 3x4 matrix와 4x5 matrix의 곱으로 [10,3,5]의 shape의 tensor를 반환한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ee8e4ac56c9e2f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## i_tensor_broadcasting.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "212b786f4f683dad"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:25:38.609641100Z",
     "start_time": "2023-09-17T10:25:38.595642400Z"
    }
   },
   "id": "9cc559dc57af80b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "첫 예제는 elements broadcasting에 대한 것으로 각 요소에 2씩 곱하여 결과가 나온다.\n",
    "두 번째 예제, t3은 [3,2] shape tensor와 [2] shape에 대한 tensor의 subtract에 대한 연산으로 broadcasting되어 [[0-4,1-5],[2-4,4-5],[10-4,10-5]]이 결과로 나온다.\n",
    "세 번째 예제는 값에 .을 붙여 torch.float32으로 입력된 값이다.\n",
    "그리고 각 연산에 대해서 elements 사이에 broadcasting되어 결과를 나타낸다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70c223b9f9710a90"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:32:09.732667100Z",
     "start_time": "2023-09-17T10:32:09.723668900Z"
    }
   },
   "id": "187c89bdf5e3a9b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "normalize 함수를 선언한다 후에 image에 대한 normalize를 하기 위한 것으로 추측된다.\n",
    "shape[3,28,28]의 tensor로 normalize에 대한 결과도 element 연산이기 때문에 사이즈가 유지되는 것을 볼 수 있다.\n",
    "\n",
    "그 다음 예제는 element operation에 대해서 broadcasting 연산되어 큰 차원으로 변경 되어서 연산 결과가 나오는 것을 볼 수 있다.\n",
    "\n",
    "마지막 예제 또한 broadcasting 연산 후 elements에 대한 operation 연산 결과 더 큰 차원으로 유지되는 것을 볼 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e2ee033f03a44fa"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "print(\"#\" * 50, 6)\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:01:11.529225600Z",
     "start_time": "2023-09-17T11:01:11.515225400Z"
    }
   },
   "id": "52fc7797bba4c2df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "마찬가지로 더 큰 차원으로 변환되는 것을 볼 수 있다. 보면 차원이 잘 맞지 않는 것이 보인다. broadcasting은 last to first이기 때문에 t19의 [,,4,1]이 t20의 [,1,1]이 broadcasting되어 [,,4,1]이 되고 [5,1,,]과 [,3,,] 사이에 broadcasting되어 [5,3,4,1]이 된다. 나머지 또한 큰 차원에 대해서 broadcasting 된다.\n",
    "\n",
    "pow 연산 또한 element에 대한 연산임을 볼 수 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7984f8511345c7c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## j_tensor_indexing_slicing.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c5088fd4308da0b"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]])\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]])\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:06:03.707478800Z",
     "start_time": "2023-09-17T11:06:03.683482300Z"
    }
   },
   "id": "c2b2a5301f63c6fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "slicing은 [ , ) 집합으로 마지막 값은 exclusive된다. ':'만 있는 경우 모든 값을 얘기한다.\n",
    "따라서 각 차원에 대해서 값을 뽑는데 사용된다. -1의 경우는 마지막 index를 얘기한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a93a8ce27040dfd"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:08:33.426081500Z",
     "start_time": "2023-09-17T11:08:33.418082900Z"
    }
   },
   "id": "cf97bad2904681c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1:4는 [1,4)로 1~3번째 index까지 뽑는다.\n",
    "이러한 index slicing을 통해서 특정 값을 얻어오거나 변경할 수도 있다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1632494e6fb6be8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## k_tensor_reshaping.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d3a28c668cdec4"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:10:32.311369800Z",
     "start_time": "2023-09-17T11:10:32.297371600Z"
    }
   },
   "id": "25e5a9f8cd42c56a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "첫 번째 예제로는 tensor를 같은 값으로 shape 형태를 변경하는 것이다.\n",
    "view와 reshape는 shallow copy로 값이 공유된다.\n",
    "다만 차이점으로는 view의 경우 데이터가 연속적으로 되지 않을 경우 오류가 발생한다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60aab3bfd784a77f"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:39:34.835787700Z",
     "start_time": "2023-09-17T11:39:34.774847200Z"
    }
   },
   "id": "8d2ab64fb2826e19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "unsqueeze는 해당 인자의 n+1 번째 위치에 dimension을 추가한다.\n",
    "따라서 첫 번째 print를 보면 (3,)의 1차원 tensor에서 1을 인자로 줄 경우 (3,1)로 2차원 tensor가 된다.\n",
    "flatten의 경우 여러 차원으로 되어있는 tensor를 1차원 tensor로 펼친다.\n",
    "하지만 start_dim으로 1일 시 1차원부터 나머지 end_dim까지의 flatten한다. (즉, start_dim부터 flatten한다.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a65a1bf757798821"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:24:18.468114300Z",
     "start_time": "2023-09-17T11:24:18.450117300Z"
    }
   },
   "id": "c1cf9c4f49d240a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "permute 함수의 경우 차원의 순서를 변경한다.\n",
    "(2,0,1) 일 경우에 (0,1,2)의 순서에서 변경하는 것이다.\n",
    "shape(2,3)에서 permute(1,0) 시 [[1,2,3],[4,5,6]]에서 [[1,4],[2,5],[3,6]]으로 shape(3,2)로 된다.\n",
    "transpose는 선형대수학의 transpose와 같다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12c3eab242bbd3b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I_tensor_concat.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc7e2a074b9e9b15"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:37:59.558415600Z",
     "start_time": "2023-09-17T11:37:59.518410900Z"
    }
   },
   "id": "8b5c283afbb1fcbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "concat과 cat은 같은 함수이다.\n",
    "해당 함수는 인자로 들어온 dim에서 연결을 하여 하나의 tensor로 만든다.\n",
    "2번째 예제에서 보듯이 dim에 있는 차원이 아닌 곳의 차원은 유지가 된다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "787217620c93fe1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## m_tensor_stacking.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7234a2759e184d"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:38:56.073772Z",
     "start_time": "2023-09-17T11:38:56.059774Z"
    }
   },
   "id": "f9075b04c2a27390"
  },
  {
   "cell_type": "markdown",
   "source": [
    "stack 함수의 경우 unsqueeze + concat이 합쳐진 것이다.\n",
    "따라서 stack 으로 만든 tensor와 unsqueeze + concat한 것으로 같은 값이 나오는 것이 보인다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa05a5bd3aaa5fb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## n_tensor_vstack_hstack.py"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f95589d23006a1f3"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:53:44.998192200Z",
     "start_time": "2023-09-17T11:53:44.951167800Z"
    }
   },
   "id": "f0d41502bec93b07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "vstack은 row-wise(vertically)로 stack을 실행하는 것이고, hstack은 col-wise(horizontally)로 stack을 실행하는 것이다.\n",
    "기본적으로 torch는 (row, col)으로 볼 수 있다. \n",
    "따라서 vstack으로 하면 row에서 합쳐지고, hstack을 할 경우 col에서 합쳐진다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb43008af00e6183"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
