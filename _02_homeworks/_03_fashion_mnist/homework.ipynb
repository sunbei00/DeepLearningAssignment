{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Full Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d0ea7e907015445"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init Setting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f87f283457ebd6"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KwonSungMin\\Desktop\\딥러닝\\link_dl\n",
      "C:/Users/KwonSungMin/Desktop\n",
      "C:/Users/KwonSungMin/Desktop\\checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "# FILE_PATH = str(Path('./').resolve())\n",
    "FILE_PATH = 'C:/Users/KwonSungMin/Desktop'\n",
    "BASE_PATH = str(Path('../../').resolve())\n",
    "\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "CHECKPOINT_FILE_PATH = os.path.join(FILE_PATH, \"checkpoints\")\n",
    "  \n",
    "print(BASE_PATH)\n",
    "print(FILE_PATH)\n",
    "print(CHECKPOINT_FILE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:54:52.787290900Z",
     "start_time": "2023-11-16T16:54:50.263142200Z"
    }
   },
   "id": "5d845e1edb80f26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DATA String "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41965eead29b7402"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from string import Template\n",
    "class DeltaTemplate(Template):\n",
    "    delimiter = \"%\"\n",
    "\n",
    "    def strfdelta(tdelta, fmt):\n",
    "        d = {\"D\": tdelta.days}\n",
    "        d[\"H\"], rem = divmod(tdelta.seconds, 3600)\n",
    "        d[\"M\"], d[\"S\"] = divmod(rem, 60)\n",
    "        t = DeltaTemplate(fmt)\n",
    "        return t.substitute(**d)\n",
    "def strfdelta(td, fmt):\n",
    "\n",
    "    # Get the timedelta’s sign and absolute number of seconds.\n",
    "    sign = \"-\" if td.days < 0 else \"+\"\n",
    "    secs = abs(td).total_seconds()\n",
    "\n",
    "    # Break the seconds into more readable quantities.\n",
    "    days, rem = divmod(secs, 86400)  # Seconds per day: 24 * 60 * 60\n",
    "    hours, rem = divmod(rem, 3600)  # Seconds per hour: 60 * 60\n",
    "    mins, secs = divmod(rem, 60)\n",
    "\n",
    "    # Format (as per above answers) and return the result string.\n",
    "    t = DeltaTemplate(fmt)\n",
    "    return t.substitute(\n",
    "        s=sign,\n",
    "        D=\"{:d}\".format(int(days)),\n",
    "        H=\"{:02d}\".format(int(hours)),\n",
    "        M=\"{:02d}\".format(int(mins)),\n",
    "        S=\"{:02d}\".format(int(secs)),\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:54:52.798290100Z",
     "start_time": "2023-11-16T16:54:52.783292300Z"
    }
   },
   "id": "84223aabb9b69adf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arg Parser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0db8c9e07bb650d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Wandb: True or False\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-b\", \"--batch_size\", type=int, default=2_048, help=\"Batch size (int, default: 2_048)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-e\", \"--epochs\", type=int, default=10_000, help=\"Number of training epochs (int, default:10_000)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-r\", \"--learning_rate\", type=float, default=1e-3, help=\"Learning rate (float, default: 1e-3)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-v\", \"--validation_intervals\", type=int, default=10,\n",
    "    help=\"Number of training epochs between validations (int, default: 10)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-p\", \"--early_stop_patience\", type=int, default=10,\n",
    "    help=\"Number of early stop patience (int, default: 10)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-d\", \"--early_stop_delta\", type=float, default=0.00001,\n",
    "    help=\"Delta value of early stop (float, default: 0.00001)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-w\", \"--weight_decay\", type=float, default=0.0, help=\"Weight decay (float, default: 0.0)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"--dropout\", action=argparse.BooleanOptionalAction, default=False, help=\"Dropout: True or False\"\n",
    "  )\n",
    "\n",
    "  return parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:54:52.842372Z",
     "start_time": "2023-11-16T16:54:52.798290100Z"
    }
   },
   "id": "20db607c236a44b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a518b777238c4d73"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:54:52.843290800Z",
     "start_time": "2023-11-16T16:54:52.813292800Z"
    }
   },
   "outputs": [],
   "source": [
    "from _01_code._99_common_utils.utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "mean = 0.0\n",
    "variance = 0.0\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "  data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "  f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "  f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "  print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "  print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "  print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "  num_data_loading_workers =  get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "  print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "    pin_memory=True, num_workers=num_data_loading_workers\n",
    "  )\n",
    "\n",
    "  global mean, variance\n",
    "  mean = 0.0\n",
    "  num = 0\n",
    "  for i in range(len(f_mnist_train)):\n",
    "      image, label = f_mnist_train[i]\n",
    "      mean += image.sum()  \n",
    "      num += image.numel() \n",
    "  mean /= num\n",
    "  for i in range(len(f_mnist_train)):\n",
    "    image, label = f_mnist_train[i]\n",
    "    variance += ((image - mean) ** 2).sum()\n",
    "  variance /= num\n",
    "\n",
    "  validation_data_loader = DataLoader(\n",
    "    dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "    pin_memory=True, num_workers=num_data_loading_workers\n",
    "  )\n",
    "\n",
    "  f_mnist_transforms = nn.Sequential(\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(mean=mean, std=variance),\n",
    "  )\n",
    "\n",
    "  return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "  data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "  f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "  f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "  print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "  print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "  test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "  global mean, variance\n",
    "  f_mnist_transforms = nn.Sequential(\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(mean=mean, std=variance),\n",
    "  )\n",
    "\n",
    "  return f_mnist_test_images, test_data_loader, f_mnist_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3dc12919790c61f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  class MyModel(nn.Module):\n",
    "    def __init__(self, in_channels, n_output):\n",
    "      super().__init__()\n",
    "      self.model = nn.Sequential(\n",
    "        # batch*1*28*28\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=(5, 5), stride=(1, 1)),\n",
    "        # batch*8*24*24\n",
    "        nn.BatchNorm2d(num_features=8),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        # batch*8*12*12\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(5, 5), stride=(1, 1)),\n",
    "        # batch16*8*8\n",
    "        nn.BatchNorm2d(num_features=16),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        # batch*16*4*4\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=256, out_features=128),\n",
    "        nn.BatchNorm1d(num_features=128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=128, out_features=64),\n",
    "        nn.BatchNorm1d(num_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=64, out_features=n_output),\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.model(x)\n",
    "      return x\n",
    "      \n",
    "  my_model = MyModel(in_channels=1, n_output=10)\n",
    "  return my_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:54:52.844290800Z",
     "start_time": "2023-11-16T16:54:52.829291400Z"
    }
   },
   "id": "f71b2a0759c7bc04"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Early stopping & Model saving"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2276ce6abe3b8b24"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  def __init__(self, patience=10, delta=0.00001, project_name=None, checkpoint_file_path=None, run_time_str=None):\n",
    "    self.patience = patience\n",
    "    self.counter = 0\n",
    "    self.delta = delta\n",
    "\n",
    "    self.val_loss_min = None\n",
    "    \n",
    "    if not os.path.isdir(checkpoint_file_path):\n",
    "      os.makedirs(checkpoint_file_path)\n",
    "      \n",
    "    self.file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "    )\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "  def check_and_save(self, new_validation_loss, model):\n",
    "    early_stop = False\n",
    "        \n",
    "    if self.val_loss_min is None:\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      message = f'Early stopping is stated!'\n",
    "    elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "      message = f'V_loss decreased ({self.val_loss_min:7.5f} --> {new_validation_loss:7.5f}). Saving model...'\n",
    "      self.save_checkpoint(new_validation_loss, model)\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      self.counter = 0\n",
    "    else:\n",
    "      self.counter += 1\n",
    "      message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "      if self.counter >= self.patience:\n",
    "        early_stop = True\n",
    "        message += \" *** TRAIN EARLY STOPPED! ***\"\n",
    "    return message, early_stop\n",
    "\n",
    "  def save_checkpoint(self, val_loss, model):\n",
    "    torch.save(model.state_dict(), self.file_path)\n",
    "    torch.save(model.state_dict(), self.latest_file_path)\n",
    "    self.val_loss_min = val_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:54:52.861290700Z",
     "start_time": "2023-11-16T16:54:52.845291200Z"
    }
   },
   "id": "f4472168e2c1820a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76a5c5411c7b64a8"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "\n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train() \n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      if self.transforms:\n",
    "        input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()  \n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        if self.transforms:\n",
    "          input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      delta=self.wandb.config.early_stop_delta,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()\n",
    "\n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:7.5f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:7.5f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:54:52.897289800Z",
     "start_time": "2023-11-16T16:54:52.861290700Z"
    }
   },
   "id": "b782100a14c34afe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Entry point"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44f36a1438b8da3d"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "  config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'validation_intervals': args.validation_intervals,\n",
    "    'learning_rate': args.learning_rate,\n",
    "    'early_stop_patience': args.early_stop_patience,\n",
    "    'early_stop_delta': args.early_stop_delta,\n",
    "    'weight_decay': args.weight_decay,\n",
    "  }\n",
    "\n",
    "  project_name = \"homework03_mnist\"\n",
    "  wandb.init(\n",
    "    mode=\"online\" if args.wandb else \"disabled\",\n",
    "    project=project_name,\n",
    "    notes=\"2019136011 homework03\",\n",
    "    tags=[\"cnn\", \"mnist\"],\n",
    "    name=run_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  print(args)\n",
    "  print(wandb.config)\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"Training on device {device}.\")\n",
    "\n",
    "  train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "  print()\n",
    "  f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "  \n",
    "  print(\"------------------------\")\n",
    "  print(\"mean : \", mean)\n",
    "  print(\"variance\", variance)\n",
    "  print(\"------------------------\")\n",
    "\n",
    "  model = get_model()\n",
    "  model.to(device)\n",
    "  wandb.watch(model)\n",
    "\n",
    "  summary(model=model, input_size=(1, 1, 28, 28))\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "  classification_trainer = ClassificationTrainer(\n",
    "    project_name, model, optimizer, train_data_loader, validation_data_loader, f_mnist_transforms,\n",
    "    run_time_str, wandb, device, CHECKPOINT_FILE_PATH\n",
    "  )\n",
    "  classification_trainer.train_loop()\n",
    "\n",
    "  wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T16:54:52.897289800Z",
     "start_time": "2023-11-16T16:54:52.877291Z"
    }
   },
   "id": "553b0b6063091289"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Entry"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f257d0fabfa877a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33m00kwonsm\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\KwonSungMin\\Desktop\\딥러닝\\link_dl\\_02_homeworks\\_03_fashion_mnist\\wandb\\run-20231117_015454-lq836vaw</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/00kwonsm/homework03_mnist/runs/lq836vaw' target=\"_blank\">2023-11-17_01-54-52</a></strong> to <a href='https://wandb.ai/00kwonsm/homework03_mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/00kwonsm/homework03_mnist' target=\"_blank\">https://wandb.ai/00kwonsm/homework03_mnist</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/00kwonsm/homework03_mnist/runs/lq836vaw' target=\"_blank\">https://wandb.ai/00kwonsm/homework03_mnist/runs/lq836vaw</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(wandb=True, batch_size=64, epochs=10000, learning_rate=0.0001, validation_intervals=10, early_stop_patience=10, early_stop_delta=1e-05, weight_decay=0.005, dropout=False)\n",
      "{'epochs': 10000, 'batch_size': 64, 'validation_intervals': 10, 'learning_rate': 0.0001, 'early_stop_patience': 10, 'early_stop_delta': 1e-05, 'weight_decay': 0.005}\n",
      "Training on device cuda:0.\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 12\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "------------------------\n",
      "mean :  tensor(0.2862)\n",
      "variance tensor(0.1247)\n",
      "------------------------\n",
      "[Epoch   1] T_loss: 1.18109, T_accuracy: 65.0964 | V_loss: 0.76662, V_accuracy: 74.5400 | Early stopping is stated! | T_time: 00:00:30, T_speed: 0.033\n",
      "[Epoch  10] T_loss: 0.39933, T_accuracy: 85.9127 | V_loss: 0.34789, V_accuracy: 87.6000 | V_loss decreased (0.76662 --> 0.34789). Saving model... | T_time: 00:04:01, T_speed: 0.041\n",
      "[Epoch  20] T_loss: 0.34579, T_accuracy: 87.9364 | V_loss: 0.29836, V_accuracy: 89.3400 | V_loss decreased (0.34789 --> 0.29836). Saving model... | T_time: 00:07:48, T_speed: 0.043\n",
      "[Epoch  30] T_loss: 0.32107, T_accuracy: 88.9018 | V_loss: 0.27640, V_accuracy: 90.5200 | V_loss decreased (0.29836 --> 0.27640). Saving model... | T_time: 00:11:32, T_speed: 0.043\n",
      "[Epoch  40] T_loss: 0.30282, T_accuracy: 89.6436 | V_loss: 0.26226, V_accuracy: 90.8000 | V_loss decreased (0.27640 --> 0.26226). Saving model... | T_time: 00:15:22, T_speed: 0.043\n",
      "[Epoch  50] T_loss: 0.29439, T_accuracy: 89.8545 | V_loss: 0.25746, V_accuracy: 90.9600 | V_loss decreased (0.26226 --> 0.25746). Saving model... | T_time: 00:19:09, T_speed: 0.044\n",
      "[Epoch  60] T_loss: 0.28498, T_accuracy: 90.2600 | V_loss: 0.24354, V_accuracy: 91.3800 | V_loss decreased (0.25746 --> 0.24354). Saving model... | T_time: 00:23:00, T_speed: 0.043\n",
      "[Epoch  70] T_loss: 0.27631, T_accuracy: 90.5491 | V_loss: 0.24740, V_accuracy: 91.4600 | Early stopping counter: 1 out of 10 | T_time: 00:26:45, T_speed: 0.044\n",
      "[Epoch  80] T_loss: 0.27222, T_accuracy: 90.7255 | V_loss: 0.23583, V_accuracy: 91.7200 | V_loss decreased (0.24354 --> 0.23583). Saving model... | T_time: 00:30:30, T_speed: 0.044\n",
      "[Epoch  90] T_loss: 0.26990, T_accuracy: 90.7236 | V_loss: 0.23527, V_accuracy: 92.0000 | V_loss decreased (0.23583 --> 0.23527). Saving model... | T_time: 00:34:11, T_speed: 0.044\n",
      "[Epoch 100] T_loss: 0.26487, T_accuracy: 90.8309 | V_loss: 0.23911, V_accuracy: 91.6000 | Early stopping counter: 1 out of 10 | T_time: 00:37:48, T_speed: 0.044\n",
      "[Epoch 110] T_loss: 0.26081, T_accuracy: 91.0382 | V_loss: 0.24549, V_accuracy: 91.5000 | Early stopping counter: 2 out of 10 | T_time: 00:41:25, T_speed: 0.044\n",
      "[Epoch 120] T_loss: 0.26051, T_accuracy: 91.0273 | V_loss: 0.24259, V_accuracy: 91.2600 | Early stopping counter: 3 out of 10 | T_time: 00:45:01, T_speed: 0.044\n",
      "[Epoch 130] T_loss: 0.25774, T_accuracy: 91.1200 | V_loss: 0.22874, V_accuracy: 92.1800 | V_loss decreased (0.23527 --> 0.22874). Saving model... | T_time: 00:48:38, T_speed: 0.045\n",
      "[Epoch 140] T_loss: 0.25448, T_accuracy: 91.3345 | V_loss: 0.22831, V_accuracy: 92.3400 | V_loss decreased (0.22874 --> 0.22831). Saving model... | T_time: 00:52:15, T_speed: 0.045\n",
      "[Epoch 150] T_loss: 0.25285, T_accuracy: 91.3109 | V_loss: 0.23029, V_accuracy: 91.8200 | Early stopping counter: 1 out of 10 | T_time: 00:55:51, T_speed: 0.045\n",
      "[Epoch 160] T_loss: 0.25343, T_accuracy: 91.2855 | V_loss: 0.23713, V_accuracy: 91.7600 | Early stopping counter: 2 out of 10 | T_time: 00:59:28, T_speed: 0.045\n",
      "[Epoch 170] T_loss: 0.24993, T_accuracy: 91.4564 | V_loss: 0.23509, V_accuracy: 91.4600 | Early stopping counter: 3 out of 10 | T_time: 01:03:04, T_speed: 0.045\n",
      "[Epoch 180] T_loss: 0.24960, T_accuracy: 91.3873 | V_loss: 0.24027, V_accuracy: 91.6600 | Early stopping counter: 4 out of 10 | T_time: 01:06:41, T_speed: 0.045\n",
      "[Epoch 190] T_loss: 0.24574, T_accuracy: 91.6145 | V_loss: 0.23501, V_accuracy: 91.7200 | Early stopping counter: 5 out of 10 | T_time: 01:10:17, T_speed: 0.045\n",
      "[Epoch 200] T_loss: 0.24783, T_accuracy: 91.5982 | V_loss: 0.22911, V_accuracy: 92.2600 | Early stopping counter: 6 out of 10 | T_time: 01:13:54, T_speed: 0.045\n",
      "[Epoch 210] T_loss: 0.24538, T_accuracy: 91.7382 | V_loss: 0.23496, V_accuracy: 91.8000 | Early stopping counter: 7 out of 10 | T_time: 01:17:31, T_speed: 0.045\n",
      "[Epoch 220] T_loss: 0.24570, T_accuracy: 91.6709 | V_loss: 0.22582, V_accuracy: 91.9800 | V_loss decreased (0.22831 --> 0.22582). Saving model... | T_time: 01:21:08, T_speed: 0.045\n",
      "[Epoch 230] T_loss: 0.24444, T_accuracy: 91.7055 | V_loss: 0.23150, V_accuracy: 91.5600 | Early stopping counter: 1 out of 10 | T_time: 01:24:44, T_speed: 0.045\n",
      "[Epoch 240] T_loss: 0.24150, T_accuracy: 91.6964 | V_loss: 0.22278, V_accuracy: 92.2400 | V_loss decreased (0.22582 --> 0.22278). Saving model... | T_time: 01:28:21, T_speed: 0.045\n",
      "[Epoch 250] T_loss: 0.24249, T_accuracy: 91.7764 | V_loss: 0.22576, V_accuracy: 91.9800 | Early stopping counter: 1 out of 10 | T_time: 01:31:57, T_speed: 0.045\n",
      "[Epoch 260] T_loss: 0.24412, T_accuracy: 91.5745 | V_loss: 0.22790, V_accuracy: 91.8400 | Early stopping counter: 2 out of 10 | T_time: 01:35:33, T_speed: 0.045\n",
      "[Epoch 270] T_loss: 0.24107, T_accuracy: 91.9309 | V_loss: 0.22045, V_accuracy: 92.3200 | V_loss decreased (0.22278 --> 0.22045). Saving model... | T_time: 01:39:09, T_speed: 0.045\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  parser = get_parser()\n",
    "  \n",
    "  sys.argv = [\"script.py\", \"--wandb\", \"-b\", \"64\", \"-r\", \"1e-4\", \"-v\", \"10\", \"-w\", \"0.005\"]\n",
    "  args = parser.parse_args()\n",
    "  main(args)  \n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-16T16:54:52.892290300Z"
    }
   },
   "id": "78c92e9382c769b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wandb login"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f627a514c8f1a59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d57be7a605a77"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8acd88d9c9a1c1d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 01"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4c0d7f1d9ac6541"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./images/problem01_code.png)\n",
    "\n",
    "![](./images/problem01.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a015215ce1f7402a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "mean과 variance는 train 이미지 데이터 셋을 기준으로 구한다.\n",
    "float의 부동소수점 문제로 인하여 data load를 할 때마다 매번 달라진다. \n",
    "하지만 일반적으로는 위와 같은 결과로 나온다.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a5d8f6c33e2a30e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 02"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6202211600b334b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2536a589066ab102"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fd23221a317fc099"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1ca1171468de03d9"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "15921860ca531aaf"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9f7ff81af386d15f"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b02dfdab09b015cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 숙제 후기"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1903ce6b4c08ec17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Accuracy 91%를 넘기기 위해서 여러 실험을 하기도 하고, 선배님들께 여쭤보기도 했다.\n",
    "작업관리자로 GPU 사용률을 확인해보았을 때 생각보다 GPU 자원을 얼마 쓰지 않았다.\n",
    "이에 대해서 여쭤보니까 일반적으로 local 환경에 맞춰서 자원을 대부분 쓰게끔 네트워크 모델을 짜는게 첫 번째 단계라고 한다.\n",
    "그리고 최적화 방법을 여러가지로 시도해 본 다음에 learning rate 등 하이퍼 파라미터를 수정해보면서 테스트 해보는게 좋을 것 같다고 하셨다.\n",
    "이를 통해서 GPU의 자원을 대부분 쓸 수 있게 네트워크 모델을 크게 만들고 이에 대해서 많은 테스트를 진행할 수 있었다.\n",
    "\n",
    "느낀점으로는 매번 금방 결과가 나오던 프로젝트만 진행하다가 이번 과제를 하면서 training을 하며 시간이 너무 오래 걸려서 답답함이 컸었다. \n",
    "\n",
    "pooling 시 feature 수도 2배하는게 일반적\n",
    "\n",
    "batch 사이즈가 너무 크면 너무 다양한 모습을 학습하려고 해서 학습에 좋지 않는다고 생각\n",
    "\n",
    "conv pooling conv pooling\n",
    "conv conv pooling\n",
    "\n",
    "image resolution에서는 자세한 특징점을 찾기 위해서 같은 이미지 h*w로 유지하지만 mnist같이 classification할 때는 어느정도 conv로 feature h*w를 줄이고서 하는게 성능이 더 좋을 것 같음\n",
    "\n",
    "task가 작을 경우파라미터 너무 많으면 오히려 좋지 않을 수 있다고 하심 \n",
    "\n",
    "\n",
    "activation function 이전에 max pooling 이후에 max pooling 차이\n",
    "\n",
    "Pooling은 일반적으로 Convolutional Neural Network (CNN) 아키텍처에서 activation function 이후에 적용됩니다. 이것은 일반적인 CNN 아키텍처의 구성에 대한 설명입니다. 이유는 다음과 같습니다.\n",
    "\n",
    "특징 추출:\n",
    "\n",
    "먼저, CNN은 입력 이미지에서 특징을 추출하기 위해 Convolutional Layer를 사용합니다. Convolutional Layer는 입력 이미지를 필터(또는 커널)와 합성곱 연산을 수행하여 특징 맵을 생성합니다. 이 과정은 비선형성을 도입하지 않습니다. 즉, 활성화 함수를 적용하지 않습니다.\n",
    "이 단계에서는 이미지의 공간적 특징을 감지하고, 각 피쳐 맵은 입력 이미지의 특정 패턴 또는 특징을 나타냅니다.\n",
    "비선형성 추가:\n",
    "\n",
    "특징 추출 후, CNN은 비선형성을 도입하기 위해 활성화 함수를 적용합니다. 일반적으로 ReLU (Rectified Linear Unit)나 다른 활성화 함수를 사용하여 특징 맵의 비선형성을 강화합니다.\n",
    "이러한 비선형성은 네트워크가 더 복잡한 패턴을 학습하고 추상화할 수 있도록 돕습니다.\n",
    "공간 축소 (Pooling):\n",
    "\n",
    "특징 맵에 비선형성이 도입된 후, 일반적으로 Pooling Layer가 적용됩니다. Pooling은 특징 맵의 공간적 크기를 줄이는 역할을 합니다.\n",
    "Pooling은 피쳐 맵 내의 작은 영역에서 최댓값(Max Pooling) 또는 평균값(Average Pooling)을 추출하여 피쳐 맵의 크기를 감소시킵니다.\n",
    "이렇게 공간 축소를 수행하면 연산량을 감소시키고, 불필요한 세부 정보를 제거하여 모델이 특징을 보다 강인하게 추출하고 계산 효율성을 향상시킵니다.\n",
    "요약하면, CNN 아키텍처에서는 특징 추출과 비선형성을 먼저 수행하고, 그 다음에 Pooling을 통해 특징 맵의 공간 크기를 축소시키는 것이 일반적입니다. 이렇게 함으로써 모델은 입력 데이터로부터 유용한 특징을 추출하고 불필요한 정보를 제거하면서 효율적으로 학습할 수 있습니다.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa7b0aae6a61d98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "805d717e561be8e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d474baf5c7bb2a32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
