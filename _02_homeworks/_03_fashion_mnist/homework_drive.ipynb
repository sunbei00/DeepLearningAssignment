{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Init Setting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f87f283457ebd6"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KwonSungMin\\Desktop\\딥러닝\\link_dl\n",
      "C:/Users/KwonSungMin/Desktop\n",
      "C:/Users/KwonSungMin/Desktop\\checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "# FILE_PATH = str(Path('./').resolve())\n",
    "FILE_PATH = 'C:/Users/KwonSungMin/Desktop'\n",
    "BASE_PATH = str(Path('../../').resolve())\n",
    "\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "CHECKPOINT_FILE_PATH = os.path.join(FILE_PATH, \"checkpoints\")\n",
    "  \n",
    "print(BASE_PATH)\n",
    "print(FILE_PATH)\n",
    "print(CHECKPOINT_FILE_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:40:04.480147600Z",
     "start_time": "2023-11-16T10:40:04.370148300Z"
    }
   },
   "id": "5d845e1edb80f26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DATA String "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41965eead29b7402"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "from string import Template\n",
    "class DeltaTemplate(Template):\n",
    "    delimiter = \"%\"\n",
    "\n",
    "    def strfdelta(tdelta, fmt):\n",
    "        d = {\"D\": tdelta.days}\n",
    "        d[\"H\"], rem = divmod(tdelta.seconds, 3600)\n",
    "        d[\"M\"], d[\"S\"] = divmod(rem, 60)\n",
    "        t = DeltaTemplate(fmt)\n",
    "        return t.substitute(**d)\n",
    "def strfdelta(td, fmt):\n",
    "\n",
    "    # Get the timedelta’s sign and absolute number of seconds.\n",
    "    sign = \"-\" if td.days < 0 else \"+\"\n",
    "    secs = abs(td).total_seconds()\n",
    "\n",
    "    # Break the seconds into more readable quantities.\n",
    "    days, rem = divmod(secs, 86400)  # Seconds per day: 24 * 60 * 60\n",
    "    hours, rem = divmod(rem, 3600)  # Seconds per hour: 60 * 60\n",
    "    mins, secs = divmod(rem, 60)\n",
    "\n",
    "    # Format (as per above answers) and return the result string.\n",
    "    t = DeltaTemplate(fmt)\n",
    "    return t.substitute(\n",
    "        s=sign,\n",
    "        D=\"{:d}\".format(int(days)),\n",
    "        H=\"{:02d}\".format(int(hours)),\n",
    "        M=\"{:02d}\".format(int(mins)),\n",
    "        S=\"{:02d}\".format(int(secs)),\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:40:04.481147700Z",
     "start_time": "2023-11-16T10:40:04.391147100Z"
    }
   },
   "id": "84223aabb9b69adf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arg Parser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0db8c9e07bb650d"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "  parser = argparse.ArgumentParser()\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Wandb: True or False\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-b\", \"--batch_size\", type=int, default=2_048, help=\"Batch size (int, default: 2_048)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-e\", \"--epochs\", type=int, default=500, help=\"Number of training epochs (int, default:10_000)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-r\", \"--learning_rate\", type=float, default=1e-3, help=\"Learning rate (float, default: 1e-3)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-v\", \"--validation_intervals\", type=int, default=10,\n",
    "    help=\"Number of training epochs between validations (int, default: 10)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-p\", \"--early_stop_patience\", type=int, default=10,\n",
    "    help=\"Number of early stop patience (int, default: 10)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-d\", \"--early_stop_delta\", type=float, default=0.00001,\n",
    "    help=\"Delta value of early stop (float, default: 0.00001)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"-w\", \"--weight_decay\", type=float, default=0.0, help=\"Weight decay (float, default: 0.0)\"\n",
    "  )\n",
    "\n",
    "  parser.add_argument(\n",
    "    \"--dropout\", action=argparse.BooleanOptionalAction, default=False, help=\"Dropout: True or False\"\n",
    "  )\n",
    "\n",
    "  return parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:40:04.484147Z",
     "start_time": "2023-11-16T10:40:04.404150800Z"
    }
   },
   "id": "20db607c236a44b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a518b777238c4d73"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:40:04.484147Z",
     "start_time": "2023-11-16T10:40:04.428147300Z"
    }
   },
   "outputs": [],
   "source": [
    "from _01_code._99_common_utils.utils import get_num_cpu_cores, is_linux, is_windows\n",
    "\n",
    "mean = 0.0\n",
    "variance = 0.0\n",
    "\n",
    "def get_fashion_mnist_data():\n",
    "  data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "  f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "  f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "  print(\"Num Train Samples: \", len(f_mnist_train))\n",
    "  print(\"Num Validation Samples: \", len(f_mnist_validation))\n",
    "  print(\"Sample Shape: \", f_mnist_train[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "  num_data_loading_workers =  get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "  print(\"Number of Data Loading Workers:\", num_data_loading_workers)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "    pin_memory=True, num_workers=num_data_loading_workers\n",
    "  )\n",
    "\n",
    "  global mean, variance\n",
    "  mean = 0.0\n",
    "  num = 0\n",
    "  for i in range(len(f_mnist_train)):\n",
    "      image, label = f_mnist_train[i]\n",
    "      mean += image.sum()  \n",
    "      num += image.numel() \n",
    "  mean /= num\n",
    "  for i in range(len(f_mnist_train)):\n",
    "    image, label = f_mnist_train[i]\n",
    "    variance += ((image - mean) ** 2).sum()\n",
    "  variance /= num\n",
    "\n",
    "  validation_data_loader = DataLoader(\n",
    "    dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "    pin_memory=True, num_workers=num_data_loading_workers\n",
    "  )\n",
    "\n",
    "  f_mnist_transforms = nn.Sequential(\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(mean=mean, std=variance),\n",
    "  )\n",
    "\n",
    "  return train_data_loader, validation_data_loader, f_mnist_transforms\n",
    "\n",
    "\n",
    "def get_fashion_mnist_test_data():\n",
    "  data_path = os.path.join(BASE_PATH, \"_00_data\", \"j_fashion_mnist\")\n",
    "\n",
    "  f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "  f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "  print(\"Num Test Samples: \", len(f_mnist_test))\n",
    "  print(\"Sample Shape: \", f_mnist_test[0][0].shape)  # torch.Size([1, 28, 28])\n",
    "\n",
    "  test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "  global mean, variance\n",
    "  f_mnist_transforms = nn.Sequential(\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize(mean=mean, std=variance),\n",
    "  )\n",
    "\n",
    "  return f_mnist_test_images, test_data_loader, f_mnist_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3dc12919790c61f"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  class MyModel(nn.Module):\n",
    "    def __init__(self, in_channels, n_output):\n",
    "      super().__init__()\n",
    "\n",
    "      self.model = nn.Sequential(\n",
    "        # batch*1*28*28\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=(5, 5), stride=(1, 1)),\n",
    "        # batch*32*24*24\n",
    "        nn.BatchNorm2d(num_features=32),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=(1, 1)),\n",
    "        # batch*64*20*20\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        # batch*64*10*10\n",
    "        nn.BatchNorm2d(num_features=64),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=(1,1)),\n",
    "        # batch*64*10*10\n",
    "        nn.BatchNorm2d(num_features=128),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=128, out_channels=1, kernel_size=(1, 1), stride=(1, 1)),\n",
    "        # batch*1*10*10 \n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(in_features=100, out_features=n_output),\n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.model(x)\n",
    "      return x\n",
    "      \n",
    "  my_model = MyModel(in_channels=1, n_output=10)\n",
    "  return my_model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:40:04.484147Z",
     "start_time": "2023-11-16T10:40:04.444146800Z"
    }
   },
   "id": "f71b2a0759c7bc04"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Early stopping & Model saving"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2276ce6abe3b8b24"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  def __init__(self, patience=10, delta=0.00001, project_name=None, checkpoint_file_path=None, run_time_str=None):\n",
    "    self.patience = patience\n",
    "    self.counter = 0\n",
    "    self.delta = delta\n",
    "\n",
    "    self.val_loss_min = None\n",
    "    \n",
    "    if not os.path.isdir(checkpoint_file_path):\n",
    "      os.makedirs(checkpoint_file_path)\n",
    "      \n",
    "    self.file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "    )\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "  def check_and_save(self, new_validation_loss, model):\n",
    "    early_stop = False\n",
    "        \n",
    "    if self.val_loss_min is None:\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      message = f'Early stopping is stated!'\n",
    "    elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "      message = f'V_loss decreased ({self.val_loss_min:7.5f} --> {new_validation_loss:7.5f}). Saving model...'\n",
    "      self.save_checkpoint(new_validation_loss, model)\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      self.counter = 0\n",
    "    else:\n",
    "      self.counter += 1\n",
    "      message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "      if self.counter >= self.patience:\n",
    "        early_stop = True\n",
    "        message += \" *** TRAIN EARLY STOPPED! ***\"\n",
    "    return message, early_stop\n",
    "\n",
    "  def save_checkpoint(self, val_loss, model):\n",
    "    torch.save(model.state_dict(), self.file_path)\n",
    "    torch.save(model.state_dict(), self.latest_file_path)\n",
    "    self.val_loss_min = val_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:40:04.484147Z",
     "start_time": "2023-11-16T10:40:04.452147500Z"
    }
   },
   "id": "f4472168e2c1820a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76a5c5411c7b64a8"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "\n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train() \n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      if self.transforms:\n",
    "        input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()  \n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        if self.transforms:\n",
    "          input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      delta=self.wandb.config.early_stop_delta,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()\n",
    "\n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:7.5f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:7.5f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:40:04.511241100Z",
     "start_time": "2023-11-16T10:40:04.468146800Z"
    }
   },
   "id": "b782100a14c34afe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entry point"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44f36a1438b8da3d"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def main(args):\n",
    "  run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "  config = {\n",
    "    'epochs': args.epochs,\n",
    "    'batch_size': args.batch_size,\n",
    "    'validation_intervals': args.validation_intervals,\n",
    "    'learning_rate': args.learning_rate,\n",
    "    'early_stop_patience': args.early_stop_patience,\n",
    "    'early_stop_delta': args.early_stop_delta,\n",
    "    'weight_decay': args.weight_decay,\n",
    "  }\n",
    "\n",
    "  project_name = \"homework03_mnist\"\n",
    "  wandb.init(\n",
    "    mode=\"online\" if args.wandb else \"disabled\",\n",
    "    project=project_name,\n",
    "    notes=\"2019136011 homework03\",\n",
    "    tags=[\"cnn\", \"mnist\"],\n",
    "    name=run_time_str,\n",
    "    config=config\n",
    "  )\n",
    "  print(args)\n",
    "  print(wandb.config)\n",
    "\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"Training on device {device}.\")\n",
    "\n",
    "  train_data_loader, validation_data_loader, f_mnist_transforms = get_fashion_mnist_data()\n",
    "  print()\n",
    "  f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "  \n",
    "  print(\"------------------------\")\n",
    "  print(\"mean : \", mean)\n",
    "  print(\"variance\", variance)\n",
    "  print(\"------------------------\")\n",
    "\n",
    "  model = get_model()\n",
    "  model.to(device)\n",
    "  wandb.watch(model)\n",
    "\n",
    "  summary(model=model, input_size=(1, 1, 28, 28))\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "  classification_trainer = ClassificationTrainer(\n",
    "    project_name, model, optimizer, train_data_loader, validation_data_loader, f_mnist_transforms,\n",
    "    run_time_str, wandb, device, CHECKPOINT_FILE_PATH\n",
    "  )\n",
    "  classification_trainer.train_loop()\n",
    "\n",
    "  wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-16T10:40:04.538148600Z",
     "start_time": "2023-11-16T10:40:04.502147Z"
    }
   },
   "id": "553b0b6063091289"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entry"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f257d0fabfa877a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\KwonSungMin\\Desktop\\딥러닝\\link_dl\\_02_homeworks\\_03_fashion_mnist\\wandb\\run-20231116_194005-shrx7i8t</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/00kwonsm/homework03_mnist/runs/shrx7i8t' target=\"_blank\">2023-11-16_19-40-04</a></strong> to <a href='https://wandb.ai/00kwonsm/homework03_mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/00kwonsm/homework03_mnist' target=\"_blank\">https://wandb.ai/00kwonsm/homework03_mnist</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/00kwonsm/homework03_mnist/runs/shrx7i8t' target=\"_blank\">https://wandb.ai/00kwonsm/homework03_mnist/runs/shrx7i8t</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(wandb=True, batch_size=2048, epochs=500, learning_rate=0.0001, validation_intervals=10, early_stop_patience=10, early_stop_delta=1e-05, weight_decay=0.005, dropout=False)\n",
      "{'epochs': 500, 'batch_size': 2048, 'validation_intervals': 10, 'learning_rate': 0.0001, 'early_stop_patience': 10, 'early_stop_delta': 1e-05, 'weight_decay': 0.005}\n",
      "Training on device cuda:0.\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "Number of Data Loading Workers: 12\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "------------------------\n",
      "mean :  tensor(0.2860)\n",
      "variance tensor(0.1246)\n",
      "------------------------\n",
      "[Epoch   1] T_loss: 2.19239, T_accuracy: 20.5255 | V_loss: 2.09070, V_accuracy: 29.2000 | Early stopping is stated! | T_time: 00:00:56, T_speed: 0.018\n",
      "[Epoch  10] T_loss: 0.84992, T_accuracy: 69.7600 | V_loss: 0.82145, V_accuracy: 70.8800 | V_loss decreased (2.09070 --> 0.82145). Saving model... | T_time: 00:06:41, T_speed: 0.025\n",
      "[Epoch  20] T_loss: 0.64252, T_accuracy: 76.3836 | V_loss: 0.62156, V_accuracy: 77.2200 | V_loss decreased (0.82145 --> 0.62156). Saving model... | T_time: 00:12:55, T_speed: 0.026\n",
      "[Epoch  30] T_loss: 0.56175, T_accuracy: 79.7673 | V_loss: 0.53695, V_accuracy: 80.4000 | V_loss decreased (0.62156 --> 0.53695). Saving model... | T_time: 00:19:05, T_speed: 0.026\n",
      "[Epoch  40] T_loss: 0.50320, T_accuracy: 81.9673 | V_loss: 0.48066, V_accuracy: 82.4000 | V_loss decreased (0.53695 --> 0.48066). Saving model... | T_time: 00:25:21, T_speed: 0.026\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  parser = get_parser()\n",
    "  \n",
    "  sys.argv = [\"script.py\", \"--wandb\", \"-b\", \"2048\", \"-r\", \"1e-4\", \"-v\", \"10\", \"-w\", \"0.005\"]\n",
    "  args = parser.parse_args()\n",
    "  main(args)  \n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-16T10:40:04.514149600Z"
    }
   },
   "id": "78c92e9382c769b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d945c491111835de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
